import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, confusion_matrix
from sklearn.preprocessing import StandardScaler

# -----------------------------
# Load dataset
# -----------------------------
df = pd.read_csv("F:/telecom_churn_403.csv", encoding='latin-1')
df = df.dropna()

# Encode categorical variables
categorical_cols = ["Plan type", "Payment method", "Gender", "Churn status"]
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# -----------------------------
# Feature selection for regression
# -----------------------------
regression_features = [
    "Previous_Month_Usage_GB",
    "Month1_Usage",
    "Month2_Usage",
    "Month3_Usage",
    "Voice minutes used",
    "Number of SMS sent",
    "Number of international calls",
    "Number of customer service complaints",
    "Monthly data usage (GB)",
    "Average monthly bill",
    "Tenure in months",
    "Age",
    "Customer satisfaction score"
]

# Add all encoded categorical columns
encoded_cols = [col for col in df.columns if any(prefix in col for prefix in categorical_cols)]
regression_features += encoded_cols

X_reg = df[regression_features]
y_reg = df["Next_Month_Data_Usage"]

# Standardization (optional for XGBoost)
scaler_reg = StandardScaler()
X_reg_scaled = scaler_reg.fit_transform(X_reg)

# -----------------------------
# XGBoost Regression for Next Month Data Usage
# -----------------------------
xgb_reg = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    objective='reg:squarederror'
)
xgb_reg.fit(X_reg_scaled, y_reg)
y_pred_reg = xgb_reg.predict(X_reg_scaled)

# Regression metrics
mse = mean_squared_error(y_reg, y_pred_reg)
r2 = r2_score(y_reg, y_pred_reg)
mae = mean_absolute_error(y_reg, y_pred_reg)
rmse = np.sqrt(mse)

print("\n--- Next Month Data Usage Prediction (XGBoost) ---")
print("R2 :", r2)
print("MSE :", mse)
print("MAE :", mae)
print("RMSE :", rmse)

# -----------------------------
# XGBoost Classification for Churn
# -----------------------------
# churn_cols = [col for col in df.columns if "Churn status" in col]
# if len(churn_cols) != 1:
#     raise ValueError(f"Expected exactly 1 encoded churn column, found: {churn_cols}")
# churn_target = churn_cols[0]

# # Features for classification (exclude churn column)
# classification_features = [col for col in X_reg.columns if "Churn status" not in col]
# X_clf = df[classification_features]
# y_clf = df[churn_target]

# # Standardization (optional for XGBoost)
# scaler_clf = StandardScaler()
# X_clf_scaled = scaler_clf.fit_transform(X_clf)

# xgb_clf = xgb.XGBClassifier(
#     n_estimators=200,
#     max_depth=5,
#     learning_rate=0.1,
#     subsample=0.8,
#     colsample_bytree=0.8,
#     random_state=42,
#     use_label_encoder=False,
#     eval_metric='logloss'
# )
# xgb_clf.fit(X_clf_scaled, y_clf)
# y_pred_clf = xgb_clf.predict(X_clf_scaled)

# # Classification metrics
# accuracy = accuracy_score(y_clf, y_pred_clf)
# conf_matrix = confusion_matrix(y_clf, y_pred_clf)

# print("\n--- Customer Churn Classification (XGBoost) ---")
# print("Churn column used:", churn_target)
# print("Accuracy :", accuracy)
# print("Confusion Matrix:\n", conf_matrix)

# # -----------------------------
# # Feature importance for classification
# # -----------------------------
# importance_df = pd.DataFrame({
#     "Feature": classification_features,
#     "Importance": xgb_clf.feature_importances_
# }).sort_values(by="Importance", ascending=False)

# print("\n--- Features Strongly Influencing Churn ---")
# print(importance_df.head(10))  # top 10 features
